# 1.1 认识Pytorch
## Pytorch定义
基于numpy的科学计算库, 两大功能:
  - 提供GPU加速
  - 灵活性和速度
## Pytorch的基本元素操作
- pytorch之所以能使用GPU加速: tensor张量  numpy核心:ndarray
- 创建矩阵
  - 未初始化的矩阵
    - torch.empty() -> return: 脏数据
  - 初始化矩阵
    - torch.rand() -> return: 均匀分布的[0, 1)
  - 创建全零矩阵
    - torch.zeros(size, dtype)
  - 直接创建tensor张量
    - **torch.tensor([])**
  - 创建形状相同的张量
    - tensor.New_ones(size)
    - torch.randn_like()
  - 获取尺寸
    - **x.size()   用于矩阵的形状查看和排查bug**

## Pytorch基本运算
- 加法距离
  - 1. x + y
  - 2. torch.add(x, y)
  - 3. torch.add(x, y, out= )
  - 4(in_place). y.add_(x)  ==  y = y + x
- 张量切片 eg: x[:, :, 1]
- 改变形状 **torch.view()   -1: 自动进行运算**   类比===> numpy: reshape, resizi, T
- **如果只有一个元素 .item()**   
- 如果有一个列比奥的元素 .tolist()

- tensor和array之间的转化
  - tensor和array是共享底层内存空间
  - **tensor --> array: tensor.numpy()**
  - **array --> tensor: torch.from_numpy()**

- 设备转移函数  .to()   GPU: "cuda";   CPU: "cpu"

```Python
# 判断服务器上已经安装了GPU和cuda
if torch.cuda.is_available():
    # 定义设备, 将设备指定成GPU
    device = torch.device("cuda")
    # 直接在GPU上创建张量y, 在CPU上创建张量x
    x = torch.randn(1)
    y = torch.ones_like(x, device=device)
    # 将x转移到GPU上
    x = x.to(device)
    # 此时x和y都在GPU上, 才可以执行加法运算
    z = x + y
    print(z)
    # 再将z转移到CPU上进行打印
    print(z.to('cpu', torch.double))
```

# 1.2 Pytorch中的autograd
- torch.tensor
  - 解释:
    - 1.  .requires_grad = True --> 表示要进行所有操作的追踪
      2. 要进行反向传播: .backward()  前提是.requires_grad属性设置为True
      3. 计算之后的梯度存放: .grad() (累加的)
      4. 要想终止一个tensor的反向传播  .detach()
      5. 终止整个的反向传播的过程, 官方推荐:  with torch.no_grad()   -->预测阶段
      6. 用户自己定义的tensor张量  grad_fn is None

- tensor操作
  - 用户自定的张量: requires_grad=False(默认值)
  - 如果想要原地改变: requires_gard_(True)

