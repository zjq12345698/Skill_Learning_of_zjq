- 创建神经网络的步骤
  - 构建神经网络
    - 定义神经网络的类
        ```Python
        class Net(nn.Module):
          def __init__(self):
            super(Net, self).__init__()
          def forward(self, x): 正向传播过程
        ```
  - 构建数据集
    - 模拟一些数据
  - 把数据集放入神经网络进行训练
    - net(input)
  - 损失值的计算  loss
    - MSEloss()
    - .grad_fn属性
  - 反向传播求其梯度
  - 更新参数

```Python
import torch
import torch.nn as nn
import torch.nn.functional as F

# 定义网络类
class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        # 定义第一层卷积层, 输入维度=1, 输出维度=6, 卷积核大小3*3
        self.conv1 = nn.Conv2d(1, 6, 3)
        # 定义第一层卷积层, 输入维度=6, 输出维度=16, 卷积核大小3*3
        self.conv2 = nn.Conv2d(6, 16, 3)
        # 定义三层全连接神经网络
        self.fc1 = nn.Linear(16 * 6 * 6, 120)
        self.fc2 = nn.Linear(120, 84)
        self.fc3 = nn.Linear(84, 10)
    
    def forward(self, x):
        # 任意卷积层后面要加激活层, 池化层
        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))
        x = F.max_pool2d(F.relu(self.conv2(x)), 2)
        # 经过卷积层的处理后, 张量要进入全连接层, 进入前需要调整张量的形状
        x = x.view(-1, self.num_flat_features(x))
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = F.relu(self.fc3(x))
        return x
    
    def num_flat_features(self, x):
        size = x.size()[1:]
        num_flat_features = 1
        for s in size:
            num_flat_features *= s
        return num_flat_features

net = Net()

# 构建数据集并计算loss
input = torch.randn(1, 1, 32, 32)
output = net(input)
target = torch.randn(10)
target = target.view(1, -1)
criterion = nn.MSELoss()
loss = criterion(output, target)

# 更新网络参数
# 导入优化器的包, optim中含有常用的优化算法, 比如SGD, Adam等
import torch.optim as op

# 通过op创建优化器对象
optimizer = op.SGD(net.parameters(), lr=0.01)

# 梯度清零
optimizer.zero_grad()

output = net(input)
loss = criterion(output, target)

# 对损失值进行反向传播的操作
loss.backward()

# 更新网络参数
optimizer.step()
```

