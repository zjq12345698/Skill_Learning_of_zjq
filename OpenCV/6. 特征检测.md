## harris角点检测
1. 沿着水平方向、竖直方向，一个变化比较平稳，一个变化比较迅速，那它就是一个边界。
2. 无论是沿着水平方向，还是沿着水平方向，变化都比较明显，则它是角点。

cv2.cornerHarris(src, blockSize, ksize, k)
- src: 数据类型为 float32 的输入图像
- blockSize: 角点检测检测领域的窗口大小
- ksize: Sobel 求导中使用的窗口大小
- k: 角点检测方程中的自由参数,取值参数为 [0,04,0.06], 一般取0.04

```Python
img = cv2.imread('mg3.jpg')
# 转化为灰度图
gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)

# 角点检测, 返回角点响应, 每一个像素都可以计算出一个角点响应.
dst = cv2.cornerHarris(gray, blockSize=2, ksize=3, k=0.04)

# 显示角点, 设定阈值, dst.max()
img[dst > (0.01 * dst.max())] = [0, 0, 255]
cv2.imshow('img', img)
cv2.waitKey(0)
cv2.destroyAllWindows()
```
## Shi-Tomasi
- Shi-Tomasi是Harris角点检测的改进
- Harrisjiao'dian'jian'ce计算的稳定性与K值有关, 而K是一个经验值, 不太好设定最佳值
- Shi-Tomasi发现, 角点的稳定性和矩阵M的较小特征值有关, 于是直接用较小的特征值作为分数, 这样就不用调整K值了
  - Shi-Tomasi将分数公式改为如下形式: 
![image](https://user-images.githubusercontent.com/129270106/232049789-777f31f7-6fa5-4f13-8561-f4daf28cd5b3.png)
  - 和Harris一样, 如果该分数大于设定的阈值, 就认为它是一个角点
goodfeaturesToTrack(image, maxCorner, qualirtLevel, minDistance)
- maxCorner: 角点的最大数, 值为0表示无限制
- qualitLevel: 角点质量, 小于1.0的整数, 一般在0.01-0.1之间
- minDistance: 角之间最小欧式距离, 忽略小于此距离的点

```Python
img = cv2.imread('mg3.jpg')
# 转化为灰度图
gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)

# Shi-Tomasi角点检测
corners = cv2.goodFeaturesToTrack(gray, 0, 0.01, 10)
corners = np.int0(corners)

# 画出角点
for i in corners:
    # i相当于corners中每一行数据, ravel()降维
    x, y = i.ravel()
    cv2.circle(img, (x, y), 3, (0, 255, 0), -1)
print(corners.shape)
cv_show('Shi-Tomasi', img)
```

## SIFT算法
使用SIFT的步骤:
  - 创建SIFT对象: sift = cv2.xfeatures2d.SIFT_create()
  - 进行检测: kp = sift.detect(img, ...)      # kp是一个列表, 存放的是封装的Keypoint对象
  - 绘制关键点: cv2.drawKeypoints(image, keypoints, outImage[, color, flag])
  - 计算描述子: cv2.detectAndCompute(image, mask[, descriptors[, useProvidedKeypoints]])
 **关键点和描述子**
 
 关键点: 位置, 大小和方向
 
 关键点描述子: 记录了关键点周围对其有共享像素点的一组向量值, 其不受仿射变换, 光照变换等影响.描述子的作用就是进行特征匹配,在后面进行特征匹配时会用得上.
 
 ```Python
# 创建sift对象
sift = cv2.xfeatures2d.SIFT_create()

# 进行检测
kp = sift.detect(gray)

# 计算描述子
kp, des = sift.detectAndCompute(gray)

# 绘制关键点
cv2.drawKeypoints(gray, kp, img)
cv_show('img', img)
 ```
## Brute-Force匹配
Brute-Force匹配又称蛮力匹配， 将一组特征点中的每一个特征点描述符与另一组的最接近的特征点描述符匹配。
使用流程:
  1. 创建BFMatcher对象, retval = cv.BFMatcher_create([, normType[, crossCheck]]) ，参数详解如下
    - normType：NORM_L1，NORM_L2，NORM_HAMMING，NORM_HAMMING2四种可选。
    - crossCheck：默认为FALSE。如果设置为TRUE，只有当两组中特征点互相匹配时才算匹配成功。

SIFT与SUFT描述符应使用NORM_L1、NORM_L2。ORB、BRISK和BRIEF描述符应该使用NORM_HAMMING。使用ORB描述符但当WTA_K等于3或4时应该选用NORM_HAMMING2。

  2. 使用两个方法：match()或knnMatch()进行描述符匹配 
**二者的区别是match()返回最佳匹配，knnMathch()返回最佳的k个匹配。两种方法对应的画点方法也不一样**

返回变量matches是一种DMatch数据结构的列表, DMatch结构含有：
  - DMatch.distance：描述符之间的距离，越低越好。
  - DMatch.queryIdx：主动匹配的描述符组中描述符的索引。
  - DMatch.trainIdx：被匹配的描述符组中描述符的索引。
  - DMatch.imgIdx：目标图像的索引。 
### 最佳匹配 match()
cv2.BFMatcher.match( queryDescriptors, trainDescriptors[, mask] )

```Python
# 创建BF匹配器对象
bf = cv2.BFMatcher(crossCheck=True)

# 特征点描述符匹配
matches = bf.match(des1, des2)

# 距离排序
matches = sorted(matches, key = lambda x: x.distance)

# 画出前10匹配
img3 = cv2.drawMatches(img1, kp1, img2, kp2, matches[:10], None, flags=2)
cv_show('img3', img3)
```
### k最佳匹配 knnMatch()
matches = cv.DescriptorMatcher.knnMatch( queryDescriptors, trainDescriptors, k[, mask[, compactResult]] )

```Python
# 创建BG匹配器对象
bf = cv2.BFMatcher_create(crossCheck=True)

# 特征点描述符匹配
matches = bf.knnMatch(des1, des2, k=1)

# 画出前10匹配
img3 = cv2.drawMatchesKnn(img1, kp1, img2, kp2, matches[:10], None, flags=2)
cv_show('knnMatch', img3)
```

## 
